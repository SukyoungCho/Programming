(You should start with the ".Rmd" file that produced what you're reading: [hw4.Rmd](hw4.Rmd). But this HTML version is easier to read.)

# STAT 327 Homework 4
We'll grade your homework by

* opening your "hw4.Rmd" file in RStudio in a directory (folder) containing the data file(s)
* clicking "Knit HTML"
* reading the HTML output
* reading your "hw4.Rmd"

You should write R code anywhere you see an empty R code chunk. You should write English text (surrounded by doubled asterisks, `**...**`, to get **boldface type**) anywhere you see "...".

The HTML version of these instructions is easier to read than this plain text ".Rmd" file because the math notation is rendered nicely. So start by clicking "Knit HTML". (Then read this ".Rmd" file, and you'll see that it's not hard to make nice mathematical notation in R Markdown.)

Include reasonable titles and labels with each of your graphs.

Name: Sukyoung Cho

Email: scho83@wisc.edu

## Part 1: Statistical tests and confidence intervals

### Difference of two means
Regarding the "mtcars" data frame, let's investigate whether engine horsepower influences gas mileage. (For the sake of this exercise, suppose that the assumptions of the difference-of-two-means test are met. In fact, they probably are not met.)

### Make one graph consisting of three rows and one column, using the same $x$-limits, c(0, 40), and the same $y$-limits, c(0, 0.15), each time.

* On top, make a density plot with rug of mpg.
* In the middle,
    + make a density plot with rug of mpg for those cars with lower-than-median horsepower
    + add a solid red circle, twice as large as the default size, at the location of the mean of these data
    + add a label "$\bar{x}=0$" (replacing "0" with the correct value) just above the red circle (hint: see ?plotmath for the bar on $x$, ?text to create a graph label, and this [hint on labels](label.html))
* At the bottom,
    + make a density plot with rug of mpg for those cars with higher-than-or-equal-to-median horsepower
    + add a solid red circle, twice as large as the default size, at the location of the mean of these data
    + add a label "$\bar{x}=0$" (replacing "0" with the correct value) ([hint on labels](label.html))
```{r}

layout(matrix(c(1,2,3),nrow=3,ncol=1))
plot(density(mtcars$mpg),xlim=c(0,40),ylim=c(0,0.15)); rug(mtcars$mpg, lwd=2)

low_hp <- mtcars$mpg[which(mtcars$hp<median(mtcars$hp))]
mean(low_hp) #24.98667 is the value
plot(density(low_hp), xlim=c(0,40), ylim=c(0,0.15), main = expession(bar(x)==24.98667)); rug(low_hp, lwd = 2)
points(x=mean(low_hp), pch=19, col="red", cex=2)

high_hp <- mtcars$mpg[which(mtcars$hp >= median(mtcars$hp))]
mean(high_hp) #15.77059 is the value
plot(density(high_hp, xlim=c(0,40),ylim=c(0,0.15)), main = expression(bar(x)==15.77059)); rug(high_hp, lwd = 2)
points(x=mean(high_hp),0, pch=19, col="red", cex=2)
 

```

### Judging only from the graph of the two samples, describe at least two differences in the corresponding populations.

* ... 

1. The cars with higher horsepower shows a more consistent mean mpg, as the peak of density plot is so significant
2. The cars with lower horsepower have different mean and median, while the higher ones seem to do.
... 

### Find the P-value for testing $H_0: \mu_{\text{mpg for low-power cars}} - \mu_{\text{mpg for high-power cars}} = 0$ against the alternative $H_1: \mu_{\text{mpg for low-power cars}} - \mu_{\text{mpg for high-power cars}} > 0$.
```{r}

t=t.test(low_hp, high_hp, alternative = "greater"); t$p.value

```

What conclusion do you draw?

In mtcars, the difference of two means mpg in low_hp and high_hp - 24.98667 and 15.77059. With the 95% CI, the p-value is 8.082542e-07 which is a way lower than 0.05, so we can reject the null hypothesis.

## Part 2: Regression models for the price of beef

### Read the data

* Read [beef.txt](beef.txt) into a data frame. (Hint: it requires one line--see ?read.table. You may read it from a file saved in the same directory as your script, or you may read it directly from the class website.)
* Display the data frame's structure
* Display the data frame's summary
```{r}

beef <- read.table(file="beef.txt", comment.char = "%", header = T)
structure(beef)
summary(beef)

```

### A first step after reading a data set into an R data frame is to check whether the categorical variables are encoded as factors. Does the beef data set have any categorical variables that should be encoded as factors?
...

A categorical variable belong to a limited number of categories. For example, a gender only male and female - a limited numbers not continuous. It seems that beef data only has continuous variables not categorical ones.



### Multiple regression
* Make a multiple linear regression model for PBE (price of beef) depending on the other variables (including YEAR).
* Display the model summary.
* Make a residual plot of residuals vs. fitted values (both of which are in your model--you don't need to do calculations):
    + Plot points of the form $(\hat{y}_i, e_i)$
    + Include the title, "Beef: residual plot"
    + Include the $y$-axis label $e_i$ (hint: search ?plotmath for how to get the subscripted $i$)
    + Include the $x$-axis label $\hat{y}_i$ (search ?plotmath for how to get the hat on the $y$)
* Make a single plot consisting of nine residual plots in a 3x3 arrangement. Each of these nine should have points of the form $(x_{ji}, e_i)$, where $x_{ji}$ is the $i^{\text{th}}$ observation of the $j^{\text{th}}$ independent variable. All variables other than PBE are independent variables here. None of these plots requires a main title, but each should have a $y$-axis label "$e_i$" and an $x$-axis label consisting of the independent variable's name.
```{r}
bm <- lm(data = beef, PBE~CBE+CFO+YEAR+PPO+CPO+PFO+DINC+RDINC+RFP)
summary(bm)

plot(bm$residuals, bm$fitted.values, main = "Beef: residual plot", xlab = expression(hat(y[i])), ylab = expression(e[i]))

layout(matrix(c(1,2,3,4,5,6,7,8,9),nrow=3,ncol=3))
plot(bm$residuals, beef$PBE, xlab = expression(hat(y[i])), ylab = expression(e[i]))
plot(bm$residuals, beef$CBE, xlab = "Consumption of beef per capita (lbs)",  ylab = expression(e[i]))
plot(bm$residuals, beef$CFO, xlab = "Food consumption per capita index (1947-1949 = 100)",  ylab = expression(e[i]))
plot(bm$residuals, beef$CPO, xlab = "Consumption of pork per capita (lbs)", ylab = expression(e[i]))
plot(bm$residuals, beef$PPO, xlab = "Price of pork (cents/lb)", ylab = expression(e[i]))
plot(bm$residuals, beef$PFO, xlab = "Retail food price index (1947-1949 = 100)", ylab = expression(e[i]))
plot(bm$residuals, beef$DINC, xlab = "Disposable income per capita index (1947-1949 = 100)",ylab = expression(e[i]))
plot(bm$residuals, beef$RDINC, xlab = "Index of real disposable income per capita (1947-1949 = 100)", ylab = expression(e[i]))
plot(bm$residuals, beef$RFP, xlab = "Retail food price index adjusted by the CPI (1947-1949 = 100)", ylab = expression(e[i]))

```

### Simple linear regression
* Look at your model summary to find the x variable whose model coefficient is most significantly different from 0. (You don't have to write R code to find this other variable--just read your model summary.)
* Make a simple linear regression model for PBE vs. this x.
* Make a scatterplot of PBE vs. this x.
    + Add the simple regression line to your scatterplot.
    + Include a reasonable title and axis labels.
```{r}

PBEDINC=lm(PBE~DINC, data = beef)
summary(PBEDINC)
plot(beef$DINC, beef$PBE, xlab="DINC", ylab = "PBE", main = "PBE vs DINC")
points(beef$DINC, fitted(PBEDINC), type="l")

```

### Are the coefficients (y-intercept and slope in the x direction) the same for this second simple linear regression model as they are in the first multiple regression model?
...
No, the gaps in coefficients and y-intercepts of multiple regression model and simple linear model are significant. While the coefficient of bm~DINC is -2.49792219, PBEDINC is -0.5381312. For intercept, bm's is 2693.01348650 but PBEDINC's is 93.0721363.


## Part 3: Regression model including confidence bands

### Create a simulated bivariate data set consisting of n=100 $(x_i, y_i)$ pairs:
* Generate n random $x$-coordinates $x_i$ from $N(0, 1)$.
* Generate n random errors, $\epsilon_i$, from $N(0, \sigma)$, using $\sigma = 4$.
* Set $y_i = \beta_0 + \beta_1 x_i + \epsilon_i$, where $\beta_0 = 2$, $\beta_1 = 3$, and $\epsilon_i \sim N(0, 4)$. (That is, $y$ is a linear function of $x$, plus some random noise.)

(Now we have simulated data. We'll pretend that we don't know
the true y-intercept $\beta_0 = 2$, the true slope $\beta_1 = 3$, the true $\sigma=4$, or the true errors $\epsilon_i$. All we know are the data, $(x_i, y_i)$. We'll let linear regression estimate the coefficients.)

### Make a graph of the data and model:
* Make a scatterplot of the data, $(x_i, y_i)$.
* Estimate a linear regression model of the form $\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i$.
* Display a summary of the model; check that the estimated coefficients are near the true $\beta_0 = 2$ and $\beta_1 = 3$.
* Add a solid black estimated regression line to the plot.
* Add a dashed red true line (y = 2 + 3x) to the plot.
* Add dotted blue 95% pointwise confidence bands that consist, for each prediction $(x_i, \hat{y}_i)$, of a vertical confidence interval around $\hat{y}_i$ centered at $(x_i, \hat{y}_i)$; the formula is $\hat{y}_i \pm t_{n-2, \alpha/2} s_{\hat{y}_i}$, where
    + $\hat{y}_i$ is the predicted $y$ at $x = x_i$ (this is available in the model you calculated)
    + $e_i = y_i - \hat{y}_i$, the $i^{\text{th}}$ residual (this estimate of $\epsilon_i$ is available in the model you calculated)
    + $s = \sqrt{\frac{\sum_{i=1}^n e_i^2}{n - 2}}$ (this is an estimate of $\sigma$)
    + $s_{\hat{y}_i} = s \sqrt{\frac{1}{n} + \frac{(x_i - \bar{x})^2}{\sum_{i=1}^n (x_i - \bar{x})^2}}$
    + $t_{n-2, \alpha/2}$ is the number that cuts off a right-tail area .025 from a Student's $t$ distribution with $n-2$ degrees of freedom
* Add a legend identifying each of the black, red, and blue lines.

Hint: These calculations might look hard, but they go quickly with R. See Quiz 2's questions 8-11 for examples of efficiently translating sums into R code.
```{r}

n=100
b0=2
b1=3
sigma=4
x=rnorm(n)
e=rnorm(n, sd=sigma)
y= b0+ b1*x + e
m = lm(y~x)
summary(m)
plot(x,y)
abline(reg=m)
abline(2,3,col="red",lty=2)

s = sqrt(sum(m$residuals^2)/(n-2)) #residuals = ei (y-yi)
sy = s * sqrt( (x-mean(x))^2 / sum( (x-mean(x))^2 ) + 1/n)
tcri = qt(0.975, df=n-2) #t.critical value t_(n-2),a/2
upline = m$fitted.values + tcri*sy 
downline = m$fitted.values - tcri*sy
points(x, upline, col="blue")
points(x, downline, col="blue")
legend(x="topright", legend=c("Estimated Regression Line", "True Regression Line", "95% Confidence"), col = c("black","red","blue"), lty=c(1,2,3))

```
